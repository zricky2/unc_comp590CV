{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompletedA6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zricky2/unc_comp590CV/blob/master/CompletedA6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D-egDhelUfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "#!unzip tiny-imagenet-200.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XTYgZ-WBJKT",
        "colab_type": "text"
      },
      "source": [
        "# Custom dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lssQs1xJmL96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def pil_loader(image_path):\n",
        "    with open(image_path, 'rb') as f:\n",
        "        return Image.open(f).convert('RGB')\n",
        "\n",
        "\n",
        "class CustomTrainDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: the dataset folder. The folder should be structured as:\n",
        "                label1/image/500 .JPEG images\n",
        "                label2/image/500 .JPEG images\n",
        "                ...\n",
        "                label200/image/500 .JPEG images\n",
        "        \"\"\"\n",
        "        #“prefix/tiny-imagenet-200/train”\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []#(image_path, class_id)\n",
        "        self.classes = []\n",
        "\n",
        "        self.root = os.path.normpath(data_dir)\n",
        "        sortedList = sorted(os.listdir(data_dir))#return sorted list of entries in directory\n",
        "        for class_id, class_name in enumerate(sortedList):\n",
        "            self.classes.append(class_name)\n",
        "            image_path = os.path.join(self.root, class_name)\n",
        "            image_path = os.path.join(image_path, \"images\")\n",
        "            for images in os.listdir(image_path): #root/class/images\n",
        "                self.samples.append((os.path.join(image_path, images), class_id))\n",
        "                \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, index): #return (image_path, class_id)\n",
        "        \"\"\"Don't forget to apply the transform before returning.\"\"\"\n",
        "        image_path, class_id = self.samples[index]\n",
        "        sample = pil_loader(image_path)\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "        return sample, class_id\n",
        "        \n",
        "\n",
        "class CustomValDataset(Dataset):\n",
        "    \"\"\"This is the Validation Dataset.\n",
        "\n",
        "    Use this as a reference to implement you CustomTrainDataset.\n",
        "    Remember because the training data and validation data are structured differently,\n",
        "    you shouldn't directly use the code below. You should design it according to the\n",
        "    training data folder's structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.root = os.path.normpath(data_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        classes_file = os.path.join(self.root, '../wnids.txt')\n",
        "\n",
        "        self.classes = []\n",
        "        with open(classes_file, 'r') as f:\n",
        "            self.classes = f.readlines()\n",
        "            for i in range(len(self.classes)):\n",
        "                self.classes[i] = self.classes[i].strip()\n",
        "        self.classes.sort()\n",
        "\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "        images_y_file = os.path.join(self.root, 'val_annotations.txt')\n",
        "        with open(images_y_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        self.class_dict = {}\n",
        "        for line in lines:\n",
        "            cols = line.split('\\t')\n",
        "            if len(cols) < 2:\n",
        "                continue\n",
        "            img_filename = cols[0].strip()\n",
        "            img_class = cols[1].strip()\n",
        "            self.class_dict[img_filename] = img_class\n",
        "\n",
        "        self.samples = []\n",
        "        images_dir = os.path.join(self.root, 'images')\n",
        "        for _root, _dirs, files in sorted(os.walk(images_dir)):\n",
        "            for image_name in files:\n",
        "                image_path = os.path.join(_root, image_name)\n",
        "                c = self.class_dict[image_name]\n",
        "                idx = self.class_to_idx[c]\n",
        "                self.samples.append((image_path, idx))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path, target = self.samples[i]\n",
        "        sample = pil_loader(path)\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOJ3VrSDZqww",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_j3Gg5PPicB",
        "colab_type": "text"
      },
      "source": [
        "# Custom Alexnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnzU7szLmNNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomAlexnet(nn.Module):\n",
        "    \"\"\"Alexnet implementation for comp 590.\n",
        "    \n",
        "    Remember, you are required to implement alexnet using\n",
        "    nn.Conv2d instead of nn.Linear.\n",
        "    Failing to do that will lead to 0 points for this task.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=200):\n",
        "        super(CustomAlexnet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=8, stride=2, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        #self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Conv2d(256, 4096, kernel_size=6, stride=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Conv2d(4096, 4096, kernel_size=1, stride=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(4096, 200, kernel_size=1, stride=3, padding=1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        #x = self.avgpool(x)\n",
        "        \n",
        "        x = self.classifier(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr-F0CvDPn6J",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ccrOp8qV1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4jdfwg9PsQm",
        "colab_type": "text"
      },
      "source": [
        "# Train and validation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SBJqYWPnKCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, dev):\n",
        "\n",
        "    print_frequency = 10\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time, losses, top1, top5],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        images = images.to(dev)\n",
        "        target = target.to(dev)\n",
        "\n",
        "        # compute output\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_frequency == 0:\n",
        "            progress.display(i)\n",
        "        \n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, dev):\n",
        "\n",
        "    print_frequency = 10\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(val_loader),\n",
        "        [batch_time, losses, top1, top5],\n",
        "        prefix='Test: ')\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.to(dev)\n",
        "            target = target.to(dev)            \n",
        "\n",
        "            # compute output\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % print_frequency == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "        # TODO: this should also be done with the ProgressMeter\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, top5.avg, losses.avg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3P4njHIPxKB",
        "colab_type": "text"
      },
      "source": [
        "# Main block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sie1SwqnZnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "9a87ce08-9f33-4d25-88eb-acd0a74d5555"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"\n",
        "\n",
        "\n",
        "model = CustomAlexnet(200)\n",
        "\n",
        "datadir = './tiny-imagenet-200'\n",
        "\n",
        "model.to(dev)\n",
        "\n",
        "# define loss function (criterion) and optimizer\n",
        "criterion = nn.CrossEntropyLoss().to(dev)\n",
        "\n",
        "lr = 0.001\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, #SGD\n",
        "                            momentum=momentum,\n",
        "                            weight_decay=weight_decay)\n",
        "\n",
        "# Data loading code\n",
        "traindir = os.path.join(datadir, 'train')\n",
        "valdir = os.path.join(datadir, 'val')\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "\n",
        "print('=> using CustomTrainDataset')\n",
        "train_dataset = CustomTrainDataset(\n",
        "    traindir,\n",
        "    transforms.Compose([\n",
        "        # transforms.RandomResizedCrop(64),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "print('=> using CustomValDataset')\n",
        "val_dataset = CustomValDataset(\n",
        "    valdir,\n",
        "    transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "\n",
        "# The following 3 lines are for verification of your model structure\n",
        "# They should output exactly the same lines as the picture in the instruction\n",
        "print('For 3x64x64 images, the model will generate outputs in the following shapes:')\n",
        "summary(model, (3, 64, 64))\n",
        "assert train_dataset.classes == val_dataset.classes\n",
        "\n",
        "# Modify batch size to see the difference\n",
        "batch_size = 256 #64-256\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True,\n",
        "    num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False,\n",
        "    num_workers=4, pin_memory=True)\n",
        "\n",
        "best_acc1 = 0\n",
        "epochs= 20\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # train for one epoch\n",
        "    train_loss = train(train_loader, model, criterion, optimizer, epoch, dev)\n",
        "\n",
        "    # evaluate on validation set\n",
        "    acc1, acc5, val_loss = validate(val_loader, model, criterion, dev)\n",
        "    \n",
        "    # remember best acc@1 and save checkpoint\n",
        "    is_best = acc1 > best_acc1\n",
        "    best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "    # Don't forget to include model_best.pth.tar in your report\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        #'arch': getpass.getuser(),\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_acc1': best_acc1,\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }, is_best)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> using CustomTrainDataset\n",
            "=> using CustomValDataset\n",
            "For 3x64x64 images, the model will generate outputs in the following shapes:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 31, 31]          12,352\n",
            "              ReLU-2           [-1, 64, 31, 31]               0\n",
            "         MaxPool2d-3           [-1, 64, 29, 29]               0\n",
            "            Conv2d-4          [-1, 192, 29, 29]         307,392\n",
            "              ReLU-5          [-1, 192, 29, 29]               0\n",
            "         MaxPool2d-6          [-1, 192, 14, 14]               0\n",
            "            Conv2d-7          [-1, 384, 14, 14]         663,936\n",
            "              ReLU-8          [-1, 384, 14, 14]               0\n",
            "            Conv2d-9          [-1, 256, 14, 14]         884,992\n",
            "             ReLU-10          [-1, 256, 14, 14]               0\n",
            "           Conv2d-11          [-1, 256, 14, 14]         590,080\n",
            "             ReLU-12          [-1, 256, 14, 14]               0\n",
            "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
            "          Dropout-14            [-1, 256, 6, 6]               0\n",
            "           Conv2d-15           [-1, 4096, 1, 1]      37,752,832\n",
            "             ReLU-16           [-1, 4096, 1, 1]               0\n",
            "          Dropout-17           [-1, 4096, 1, 1]               0\n",
            "           Conv2d-18           [-1, 4096, 1, 1]      16,781,312\n",
            "             ReLU-19           [-1, 4096, 1, 1]               0\n",
            "           Conv2d-20            [-1, 200, 1, 1]         819,400\n",
            "================================================================\n",
            "Total params: 57,812,296\n",
            "Trainable params: 57,812,296\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 7.08\n",
            "Params size (MB): 220.54\n",
            "Estimated Total Size (MB): 227.66\n",
            "----------------------------------------------------------------\n",
            "Epoch: [0][  0/391]\tTime  0.751 ( 0.751)\tData  0.608 ( 0.608)\tLoss 5.2989e+00 (5.2989e+00)\tAcc@1   0.39 (  0.39)\tAcc@5   1.95 (  1.95)\n",
            "Epoch: [0][ 10/391]\tTime  1.122 ( 1.087)\tData  0.000 ( 0.151)\tLoss 5.2988e+00 (5.2984e+00)\tAcc@1   0.00 (  0.43)\tAcc@5   2.73 (  2.81)\n",
            "Epoch: [0][ 20/391]\tTime  1.122 ( 1.104)\tData  0.000 ( 0.128)\tLoss 5.2975e+00 (5.2985e+00)\tAcc@1   0.78 (  0.43)\tAcc@5   3.12 (  2.68)\n",
            "Epoch: [0][ 30/391]\tTime  1.132 ( 1.111)\tData  0.000 ( 0.120)\tLoss 5.2982e+00 (5.2984e+00)\tAcc@1   0.39 (  0.50)\tAcc@5   1.56 (  2.60)\n",
            "Epoch: [0][ 40/391]\tTime  1.128 ( 1.116)\tData  0.000 ( 0.116)\tLoss 5.2987e+00 (5.2983e+00)\tAcc@1   0.00 (  0.50)\tAcc@5   1.56 (  2.51)\n",
            "Epoch: [0][ 50/391]\tTime  1.133 ( 1.119)\tData  0.000 ( 0.114)\tLoss 5.2965e+00 (5.2983e+00)\tAcc@1   0.39 (  0.47)\tAcc@5   3.52 (  2.57)\n",
            "Epoch: [0][ 60/391]\tTime  1.133 ( 1.121)\tData  0.000 ( 0.112)\tLoss 5.2984e+00 (5.2983e+00)\tAcc@1   0.00 (  0.46)\tAcc@5   3.12 (  2.57)\n",
            "Epoch: [0][ 70/391]\tTime  1.132 ( 1.122)\tData  0.000 ( 0.111)\tLoss 5.2987e+00 (5.2983e+00)\tAcc@1   0.00 (  0.41)\tAcc@5   1.95 (  2.53)\n",
            "Epoch: [0][ 80/391]\tTime  1.131 ( 1.123)\tData  0.000 ( 0.110)\tLoss 5.2984e+00 (5.2984e+00)\tAcc@1   0.00 (  0.42)\tAcc@5   0.39 (  2.46)\n",
            "Epoch: [0][ 90/391]\tTime  1.124 ( 1.124)\tData  0.000 ( 0.110)\tLoss 5.2989e+00 (5.2983e+00)\tAcc@1   0.00 (  0.42)\tAcc@5   1.56 (  2.44)\n",
            "Epoch: [0][100/391]\tTime  1.132 ( 1.124)\tData  0.000 ( 0.109)\tLoss 5.2983e+00 (5.2983e+00)\tAcc@1   0.00 (  0.43)\tAcc@5   0.78 (  2.38)\n",
            "Epoch: [0][110/391]\tTime  1.126 ( 1.125)\tData  0.000 ( 0.109)\tLoss 5.2980e+00 (5.2984e+00)\tAcc@1   0.00 (  0.43)\tAcc@5   4.30 (  2.44)\n",
            "Epoch: [0][120/391]\tTime  1.133 ( 1.125)\tData  0.000 ( 0.108)\tLoss 5.2989e+00 (5.2984e+00)\tAcc@1   0.00 (  0.42)\tAcc@5   1.95 (  2.44)\n",
            "Epoch: [0][130/391]\tTime  1.132 ( 1.126)\tData  0.000 ( 0.108)\tLoss 5.2988e+00 (5.2984e+00)\tAcc@1   0.00 (  0.41)\tAcc@5   3.12 (  2.45)\n",
            "Epoch: [0][140/391]\tTime  1.131 ( 1.126)\tData  0.000 ( 0.108)\tLoss 5.2982e+00 (5.2984e+00)\tAcc@1   0.00 (  0.41)\tAcc@5   2.34 (  2.46)\n",
            "Epoch: [0][150/391]\tTime  1.133 ( 1.126)\tData  0.000 ( 0.108)\tLoss 5.2978e+00 (5.2984e+00)\tAcc@1   0.00 (  0.41)\tAcc@5   0.78 (  2.46)\n",
            "Epoch: [0][160/391]\tTime  1.130 ( 1.127)\tData  0.000 ( 0.107)\tLoss 5.2987e+00 (5.2984e+00)\tAcc@1   0.00 (  0.41)\tAcc@5   0.78 (  2.45)\n",
            "Epoch: [0][170/391]\tTime  1.127 ( 1.127)\tData  0.000 ( 0.107)\tLoss 5.2991e+00 (5.2984e+00)\tAcc@1   0.39 (  0.42)\tAcc@5   2.34 (  2.43)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}